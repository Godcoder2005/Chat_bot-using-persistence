<img width="863" height="725" alt="image" src="https://github.com/user-attachments/assets/291241e2-4bc2-4c32-852e-bb7e55e9ecd0" /><br>

This is the rough workflow of my project

## üöÄ Getting Started

Follow the steps below to set up and run the project locally.

---

### 1Ô∏è‚É£ Clone the Repository

```bash
git clone <your-github-repo-url>
cd <repo-folder>
```

---

### 2Ô∏è‚É£ Install Core Dependencies

Make sure you are using **Python 3.9+**, then install the required libraries:

```bash
pip install langgraph langchain python-dotenv
```

---

### 3Ô∏è‚É£ Choose an LLM Provider

This project is **LLM-agnostic**. You can use either **open-source models** or **cloud-based models**.

---

#### üîπ Option A: Hugging Face (Recommended ‚Äì Free & Open Source)

If you prefer open-source models from Hugging Face:

```bash
pip install langchain_huggingface
```

Configure your Hugging Face token in a `.env` file:

```
HUGGINGFACEHUB_API_TOKEN=your_token_here
```

---

#### üîπ Option B: Google Gemini

If you want to use Google Gemini:

```bash
pip install langchain_google_genai
```

Add your API key to `.env`:

```
GOOGLE_API_KEY=your_api_key_here
```

---

### 4Ô∏è‚É£ Run the Project

Once dependencies are installed and environment variables are set, you are ready to run the project.

---

### 5Ô∏è‚É£ Run with UI (Streamlit)

To launch the frontend interface:

```bash
streamlit run <frontend_ui_file.py>
```

Example:

```bash
streamlit run app.py
```

---

## ‚úÖ Notes

* The backend logic is powered by **LangGraph** for structured AI workflows.
* The project supports **multiple LLM providers** without changing core logic.
* UI is built using **Streamlit** for quick interaction and demos.

---




